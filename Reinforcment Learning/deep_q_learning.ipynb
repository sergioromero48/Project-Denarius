{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:20:05.713857: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 17:20:05.950187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 17:20:07.126970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(action, current_price, cash, stocks_held):\n",
    "    transaction_fee = 0  # Set transaction fee if needed\n",
    "    if action == 0:  # Buy\n",
    "        if cash >= (current_price + transaction_fee):\n",
    "            num_stocks = (cash - transaction_fee) / current_price\n",
    "            total_cost = num_stocks * current_price + transaction_fee\n",
    "            stocks_held += num_stocks\n",
    "            cash -= total_cost\n",
    "            reward = 0  # No immediate reward for buying\n",
    "        else:\n",
    "            reward = -100  # Penalty for insufficient cash\n",
    "    elif action == 1:  # Sell\n",
    "        if stocks_held > 0:\n",
    "            total_value = stocks_held * current_price\n",
    "            profit = total_value - transaction_fee\n",
    "            cash += profit\n",
    "            stocks_held = 0\n",
    "            reward = profit  # Reward is the profit from selling\n",
    "        else:\n",
    "            reward = -100  # Penalty for no stocks to sell\n",
    "    elif action == 2:  # Hold\n",
    "        reward = 0  # Neutral reward for holding\n",
    "    else:\n",
    "        reward = 0  # Default reward\n",
    "    return reward, cash, stocks_held\n",
    "\n",
    "def update_state(time_step, data, cash, stocks_held ):\n",
    "    \"\"\"Update the state vector with market data and raw agent-specific features.\"\"\"\n",
    "    # Extract market features for the current time step\n",
    "    market_features = data.iloc[time_step][[\n",
    "        'Close/Last', 'Open', 'High', 'Low', 'Volume',\n",
    "        'Returns', 'SMA-5', 'SMA-20', 'Volatility', 'MACD'\n",
    "    ]].values.tolist()\n",
    "\n",
    "    # Use raw agent-specific features\n",
    "    agent_features = [cash, stocks_held]  # Directly append raw cash and stocks_held values\n",
    "\n",
    "    # Combine market and agent features into a single state\n",
    "    state = market_features + agent_features\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    return state\n",
    "\n",
    "# Neural network model for approximating Q-values\n",
    "def build_model(state_size, action_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(state_size,)),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='relu'),\n",
    "        tf.keras.layers.Dense(action_size, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "def select_action_dqn(state, exploration_rate):\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        return np.random.choice(range(action_size))  # Explore\n",
    "    else:\n",
    "        state_array = np.array(state, dtype=np.float32)\n",
    "        q_values = main_model.predict(state_array[np.newaxis, :])  # Shape (1, state_size)\n",
    "        return np.argmax(q_values[0])  # Exploit\n",
    "\n",
    "def store_experience(state, action, reward, next_state, done):\n",
    "    state_array = np.array(state, dtype=np.float32)\n",
    "    next_state_array = np.array(next_state, dtype=np.float32)\n",
    "    memory.append((state_array, action, reward, next_state_array, done))\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    return random.sample(memory, batch_size)\n",
    "\n",
    "# List of Companies\n",
    "companies = ['AMD', 'AMZN', 'CSCO', 'META', 'MSFT', 'NFLX', 'QCOM', 'SBUX', 'TSLA']\n",
    "\n",
    "# Data Loading and Preprocessing Function\n",
    "def load_and_preprocess_data(company):\n",
    "    data = pd.read_csv(f'company_datasets/{company}_data.csv')\n",
    "    data = data.iloc[::-1].reset_index(drop=True)  # Earliest to latest\n",
    "    data['EMA-12'] = data['Close/Last'].ewm(span=12, adjust=False).mean()\n",
    "    data['EMA-26'] = data['Close/Last'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD'] = data['EMA-12'] - data['EMA-26']\n",
    "    data['Returns'] = data['Close/Last'].pct_change()\n",
    "    data['SMA-5'] = data['Close/Last'].rolling(window=5).mean()\n",
    "    data['SMA-20'] = data['Close/Last'].rolling(window=20).mean()\n",
    "    data['Volatility'] = data['Returns'].rolling(window=5).std()\n",
    "    data = data.dropna()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 19s 19s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 902ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 1s 779ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 958ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 827ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 55s 55s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load Data for All Companies\n",
    "company_data = {company: load_and_preprocess_data(company) for company in companies}\n",
    "\n",
    "cash_initial = 1000  # Initial cash available\n",
    "stocks_held_initial = 0  # Initial stocks held\n",
    "transaction_fee = 0\n",
    "\n",
    "# Define action space\n",
    "actions = [0, 1, 2]  # 0 = Buy, 1 = Sell, 2 = Hold\n",
    "\n",
    "# Define hyperparameters\n",
    "state_size = 12  # Length of your feature vector\n",
    "action_size = 3  # Number of possible actions\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "batch_size = 32\n",
    "episodes = 100\n",
    "exploration_rate = 0.95  # Initial exploration rate\n",
    "exploration_decay = 0.95\n",
    "exploration_min = 0.01\n",
    "memory_size = 10000  # Replay buffer \n",
    "train_frequency = 10  # Train every 10 steps\n",
    "\n",
    "# Initialize the main and target networks\n",
    "main_model = build_model(state_size, action_size)\n",
    "target_model = build_model(state_size, action_size)\n",
    "target_model.set_weights(main_model.get_weights())\n",
    "\n",
    "# Experience replay buffer\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "total_profits = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(episodes):\n",
    "    episode_start_time = time.time()\n",
    "    print(f\"Starting Episode {episode + 1}/{episodes}\")\n",
    "\n",
    "    for company in companies:\n",
    "        data = company_data[company]\n",
    "        cash = cash_initial\n",
    "        stocks_held = stocks_held_initial\n",
    "        state = update_state(0, data, cash, stocks_held)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(len(data) - 1):\n",
    "            # Select an action\n",
    "            action = select_action_dqn(state, exploration_rate)\n",
    "\n",
    "            # Perform the action\n",
    "            reward, cash, stocks_held = reward_function(action, data.iloc[t]['Close/Last'], cash, stocks_held)\n",
    "            next_state = update_state(t + 1, data, cash, stocks_held)\n",
    "            done = (t == len(data) - 2)  # Check if it's the last time step\n",
    "\n",
    "            # Store the experience\n",
    "            store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train the model at specified intervals\n",
    "            if len(memory) >= batch_size and t % train_frequency == 0:\n",
    "                batch = sample_experiences(batch_size)\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = zip(*batch)\n",
    "                states_batch = np.array(states_batch, dtype=np.float32)\n",
    "                next_states_batch = np.array(next_states_batch, dtype=np.float32)\n",
    "                rewards_batch = np.array(rewards_batch, dtype=np.float32)\n",
    "                dones_batch = np.array(dones_batch, dtype=np.float32)\n",
    "\n",
    "                # Predict Q-values for next states using the target model\n",
    "                next_q_values = target_model.predict(next_states_batch, verbose=0)\n",
    "                max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                targets = rewards_batch + discount_factor * (1 - dones_batch) * max_next_q_values\n",
    "\n",
    "                # Predict Q-values for current states\n",
    "                q_values = main_model.predict(states_batch, verbose=0)\n",
    "\n",
    "                # Update Q-values for selected actions\n",
    "                for i in range(len(batch)):\n",
    "                    q_values[i, actions_batch[i]] = targets[i]\n",
    "\n",
    "                # Train the main model\n",
    "                main_model.fit(states_batch, q_values, verbose=0, epochs=1)\n",
    "\n",
    "        # Reduce exploration rate after each company\n",
    "        exploration_rate = max(exploration_rate * exploration_decay, exploration_min)\n",
    "\n",
    "        # Update the target network periodically\n",
    "        target_model.set_weights(main_model.get_weights())\n",
    "\n",
    "        # Calculate Profit for the Company\n",
    "        final_price = data.iloc[-1]['Close/Last']\n",
    "        net_worth = cash + stocks_held * final_price\n",
    "        profit = net_worth - cash_initial\n",
    "        total_profits.append(profit)\n",
    "        print(f\"Company: {company}, Episode {episode + 1}/{episodes}, Profit: {profit}\")\n",
    "    main_model.save('trained_dqn_model.h5')\n",
    "    print(f\"Episode {episode + 1}/{episodes} completed in {time.time() - episode_start_time:.2f} seconds\")\n",
    "\n",
    "# Optionally, save the model after training\n",
    "main_model.save('trained_dqn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if crash:\n",
    "# Load Data for All Companies\n",
    "company_data = {company: load_and_preprocess_data(company) for company in companies}\n",
    "\n",
    "cash_initial = 1000  # Initial cash available\n",
    "stocks_held_initial = 0  # Initial stocks held\n",
    "transaction_fee = 0\n",
    "\n",
    "# Define action space\n",
    "actions = [0, 1, 2]  # 0 = Buy, 1 = Sell, 2 = Hold\n",
    "\n",
    "# Define hyperparameters\n",
    "state_size = 12  # Length of your feature vector\n",
    "action_size = 3  # Number of possible actions\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.9\n",
    "batch_size = 128\n",
    "episodes = 1000\n",
    "exploration_rate = 0.3  # Initial exploration rate\n",
    "exploration_decay = 0.9\n",
    "exploration_min = 0.01\n",
    "memory_size = 10000  # Replay buffer \n",
    "train_frequency = 10  # Train every 10 steps\n",
    "\n",
    "# **Load the previously saved main model**\n",
    "try:\n",
    "    # Attempt to load the model with compile=False\n",
    "    main_model = tf.keras.models.load_model('trained_dqn_model.h5', compile=False)\n",
    "except ValueError as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # If loading fails, rebuild the model architecture and load weights\n",
    "    main_model = build_model(state_size, action_size)\n",
    "    main_model.load_weights('trained_dqn_model.h5')\n",
    "\n",
    "target_model = build_model(state_size, action_size)\n",
    "target_model.set_weights(main_model.get_weights())\n",
    "\n",
    "# Experience replay buffer\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "total_profits = []\n",
    "\n",
    "# Training Loop\n",
    "for episode in range(episodes):\n",
    "    episode_start_time = time.time()\n",
    "    print(f\"Starting Episode {episode + 1}/{episodes}\")\n",
    "\n",
    "    for company in companies:\n",
    "        data = company_data[company]\n",
    "        cash = cash_initial\n",
    "        stocks_held = stocks_held_initial\n",
    "        state = update_state(0, data, cash, stocks_held)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(len(data) - 1):\n",
    "            # Select an action\n",
    "            action = select_action_dqn(state, exploration_rate)\n",
    "\n",
    "            # Perform the action\n",
    "            reward, cash, stocks_held = reward_function(action, data.iloc[t]['Close/Last'], cash, stocks_held)\n",
    "            next_state = update_state(t + 1, data, cash, stocks_held)\n",
    "            done = (t == len(data) - 2)  # Check if it's the last time step\n",
    "\n",
    "            # Store the experience\n",
    "            store_experience(state, action, reward, next_state, done)\n",
    "\n",
    "            # Update the state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Train the model at specified intervals\n",
    "            if len(memory) >= batch_size and t % train_frequency == 0:\n",
    "                batch = sample_experiences(batch_size)\n",
    "                states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch = zip(*batch)\n",
    "                states_batch = np.array(states_batch, dtype=np.float32)\n",
    "                next_states_batch = np.array(next_states_batch, dtype=np.float32)\n",
    "                rewards_batch = np.array(rewards_batch, dtype=np.float32)\n",
    "                dones_batch = np.array(dones_batch, dtype=np.float32)\n",
    "\n",
    "                # Predict Q-values for next states using the target model\n",
    "                next_q_values = target_model.predict(next_states_batch, verbose=0)\n",
    "                max_next_q_values = np.max(next_q_values, axis=1)\n",
    "\n",
    "                # Compute target Q-values\n",
    "                targets = rewards_batch + discount_factor * (1 - dones_batch) * max_next_q_values\n",
    "\n",
    "                # Predict Q-values for current states\n",
    "                q_values = main_model.predict(states_batch, verbose=0)\n",
    "\n",
    "                # Update Q-values for selected actions\n",
    "                for i in range(len(batch)):\n",
    "                    q_values[i, actions_batch[i]] = targets[i]\n",
    "\n",
    "                # Train the main model\n",
    "                main_model.fit(states_batch, q_values, verbose=0, epochs=1)\n",
    "\n",
    "        # Reduce exploration rate after each company\n",
    "        exploration_rate = max(exploration_rate * exploration_decay, exploration_min)\n",
    "\n",
    "        # Update the target network periodically\n",
    "        if episode % 5 == 0:\n",
    "            target_model.set_weights(main_model.get_weights())\n",
    "\n",
    "        # Calculate Profit for the Company\n",
    "        final_price = data.iloc[-1]['Close/Last']\n",
    "        net_worth = cash + stocks_held * final_price\n",
    "        profit = net_worth - cash_initial\n",
    "        total_profits.append(profit)\n",
    "        print(f\"Company: {company}, Episode {episode + 1}/{episodes}, Profit: {profit}\")\n",
    "    main_model.save('trained_dqn_model2.h5')\n",
    "    print(f\"Episode {episode + 1}/{episodes} completed in {time.time() - episode_start_time:.2f} seconds\")\n",
    "\n",
    "# Optionally, save the model after training\n",
    "main_model.save('trained_dqn_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 773ms/step\n",
      "1/1 [==============================] - 1s 840ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 753ms/step\n",
      "1/1 [==============================] - 1s 811ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 692ms/step\n",
      "1/1 [==============================] - 1s 737ms/step\n",
      "1/1 [==============================] - 1s 869ms/step\n",
      "1/1 [==============================] - 1s 789ms/step\n",
      "1/1 [==============================] - 1s 819ms/step\n",
      "1/1 [==============================] - 1s 719ms/step\n",
      "1/1 [==============================] - 1s 859ms/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 924ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 833ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 840ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 952ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1361511/1633458531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Greedy action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstocks_held\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close/Last'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstocks_held\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstocks_held\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1361511/3413342105.py\u001b[0m in \u001b[0;36mselect_action_dqn\u001b[0;34m(state, exploration_rate)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstate_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape (1, state_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Exploit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2347\u001b[0m                     )\n\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2349\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2350\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1261\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;31m# performance.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2238\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m       warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m     36\u001b[0m                     \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m---> 37\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    111\u001b[0m         use_legacy_function=use_legacy_function)\n\u001b[1;32m    112\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3459\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3460\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MapDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3461\u001b[0m         \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TEST 1\n",
    "# Load test data\n",
    "test_data = load_and_preprocess_data('AAPL')\n",
    "\n",
    "# Load trained model\n",
    "main_model = tf.keras.models.load_model('trained_dqn_model.h5', compile=False)\n",
    "\n",
    "# Initialize state params\n",
    "cash_init = 1000\n",
    "stocks_held_init = 0\n",
    "state_size = 12\n",
    "action_size = 3\n",
    "\n",
    "test_episodes = 100\n",
    "test_profits1 = []\n",
    "\n",
    "for episode in range(test_episodes):\n",
    "    cash = cash_init\n",
    "    stocks_held = stocks_held_init\n",
    "    state = update_state(0, test_data, cash, stocks_held)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(len(test_data) - 1):\n",
    "        action = select_action_dqn(state, 0)  # Greedy action\n",
    "        reward, cash, stocks_held = reward_function(action, test_data.iloc[t]['Close/Last'], cash, stocks_held)\n",
    "        next_state = update_state(t + 1, test_data, cash, stocks_held)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    final_price = test_data.iloc[-1]['Close/Last']\n",
    "    net_worth = cash + stocks_held * final_price\n",
    "    profit = net_worth - cash_init\n",
    "    test_profits1.append(profit)\n",
    "    print(f\"Episode {episode + 1}/{test_episodes}, Profit: {profit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.plot(test_profits1)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Profit')\n",
    "plt.title('Profit per Episode (AAPL)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
